{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install selenium\n",
    "!apt-get update\n",
    "!apt install chromium-chromedriver\n",
    "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
    "!pip install webdriver_manager\n",
    "!pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 페이지 크롤링 완료\n",
      "2 페이지 크롤링 완료\n",
      "3 페이지 크롤링 완료\n",
      "4 페이지 크롤링 완료\n",
      "5 페이지 크롤링 완료\n",
      "6 페이지 크롤링 완료\n",
      "7 페이지 크롤링 완료\n",
      "8 페이지 크롤링 완료\n",
      "9 페이지 크롤링 완료\n",
      "10 페이지 크롤링 완료\n",
      "11 페이지 크롤링 완료\n",
      "12 페이지 크롤링 완료\n",
      "13 페이지 크롤링 완료\n",
      "14 페이지 크롤링 완료\n",
      "15 페이지 크롤링 완료\n",
      "16 페이지 크롤링 완료\n",
      "17 페이지 크롤링 완료\n",
      "18 페이지 크롤링 완료\n",
      "19 페이지 크롤링 완료\n",
      "20 페이지 크롤링 완료\n",
      "21 페이지 크롤링 완료\n",
      "22 페이지 크롤링 완료\n",
      "23 페이지 크롤링 완료\n",
      "24 페이지 크롤링 완료\n",
      "25 페이지 크롤링 완료\n",
      "26 페이지 크롤링 완료\n",
      "27 페이지 크롤링 완료\n",
      "마지막 페이지에 도달하여 크롤링을 종료합니다.\n",
      "JSON 파일 저장 완료: '/Users/youyoungcheon/Desktop/django-project/crawling/peoplenjob_job_list.json'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "# 크롤링할 데이터를 저장할 리스트\n",
    "jobs = []\n",
    "\n",
    "page = 1  # 페이지 번호 초기값 설정\n",
    "while True:\n",
    "    url = f'https://www.peoplenjob.com/jobs?field=all&q=데이터&page={page}'\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    rows = soup.find_all('tr')\n",
    "\n",
    "    # 각 공고에서 필요한 정보 추출 및 리스트에 추가\n",
    "    for row in rows:\n",
    "        # 공고 제목\n",
    "        title_tag = row.find('td', class_='job-title')\n",
    "        title = title_tag.find('a').text.strip() if title_tag else 'N/A'\n",
    "\n",
    "        # 회사 이름\n",
    "        company_tag = row.find('td', class_='name')\n",
    "        company_name = company_tag.find('a').text.strip() if company_tag else 'N/A'\n",
    "\n",
    "        # 상세 페이지 URL\n",
    "        detail_url_tag = title_tag.find('a') if title_tag else None\n",
    "        detail_url = f\"https://www.peoplenjob.com{detail_url_tag['href']}\" if detail_url_tag else 'N/A'\n",
    "\n",
    "        # 마감일\n",
    "        end_date_tag = row.find('span', class_='job-fin-date')\n",
    "        end_date = end_date_tag.text.strip() if end_date_tag else 'N/A'\n",
    "\n",
    "        # 카테고리 (직무 타입)\n",
    "        category_tag = row.find('td', class_='job_type')\n",
    "        category_name = category_tag.text.strip() if category_tag else 'N/A'\n",
    "\n",
    "        # 기술 스택 (이 사이트에는 없으므로 'N/A'로 처리)\n",
    "        stack = 'N/A'\n",
    "\n",
    "        # 모든 <td> 태그를 리스트로 가져옴\n",
    "        td_tags = row.find_all('td')\n",
    "\n",
    "        # <td> 태그가 최소한 2개 이상 있는지 확인\n",
    "        if len(td_tags) >= 2:\n",
    "            region_tag = td_tags[-2].find('a')  # 두 번째 마지막 <td>에서 <a> 태그 찾기\n",
    "            region = region_tag.text.strip() if region_tag else 'N/A'\n",
    "        else:\n",
    "            region = 'N/A'  # <td> 태그가 2개 미만인 경우 N/A로 설정\n",
    "\n",
    "        # 신입/경력 (이 사이트에는 없으므로 'N/A'로 처리)\n",
    "        career = 'N/A'\n",
    "\n",
    "        # 출처 (플랫폼 이름)\n",
    "        platform_name = 'peoplenjob'\n",
    "\n",
    "        # 추출한 데이터를 딕셔너리로 추가\n",
    "        job_data = {\n",
    "            'title': title,\n",
    "            'company_name': company_name,\n",
    "            'detail_url': detail_url,\n",
    "            'end_date': end_date,\n",
    "            'platform_name': platform_name,\n",
    "            'category_name': category_name,\n",
    "            'stack': stack,\n",
    "            'region': region,\n",
    "            'career': career\n",
    "        }\n",
    "\n",
    "        # 리스트에 저장\n",
    "        jobs.append(job_data)\n",
    "\n",
    "    print(f\"{page} 페이지 크롤링 완료\")\n",
    "\n",
    "    # '다음' 버튼이 비활성화된 경우 크롤링 종료\n",
    "    next_button = soup.find('li', class_='next disabled')\n",
    "    if next_button:\n",
    "        print(\"마지막 페이지에 도달하여 크롤링을 종료합니다.\")\n",
    "        break\n",
    "\n",
    "    page += 1  # 다음 페이지로 이동\n",
    "\n",
    "# JSON 파일로 저장\n",
    "json_file = '/Users/youyoungcheon/Desktop/django-project/crawling/peoplenjob_job_list.json'\n",
    "with open(json_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(jobs, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"JSON 파일 저장 완료: '{json_file}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "더 이상 스크롤 할 수 없습니다.\n",
      "7\n",
      "JSON 파일 저장 완료: 'remember_job_list.json'\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import json\n",
    "\n",
    "# Selenium WebDriver 설정 (Chrome을 예시로 사용)\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('headless')  # 브라우저 창이 뜨지 않도록\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "# Remember 앱 URL로 이동\n",
    "url = 'https://career.rememberapp.co.kr/job/postings?search=%7B%22leaderPosition%22%3Afalse%2C%22organizationType%22%3A%22all%22%2C%22applicationType%22%3A%22all%22%2C%22keywords%22%3A%5B%22데이터%22%5D%7D'\n",
    "base_url = 'https://career.rememberapp.co.kr/'\n",
    "driver.get(url)\n",
    "\n",
    "# 페이지 로딩을 기다림\n",
    "time.sleep(3)\n",
    "\n",
    "# 데이터를 담을 리스트\n",
    "jobs = []\n",
    "\n",
    "# 스크롤을 끝까지 내리며 데이터를 로드\n",
    "last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "scroll_count = 0\n",
    "\n",
    "while True:\n",
    "    # 페이지의 HTML을 가져와서 BeautifulSoup으로 파싱\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "    # 데이터 추출 (ul 태그 내 li 태그에 각각의 공고가 있다고 가정)\n",
    "    job_list = soup.find_all('div', class_='sc-f0acadd4-0 bYquQe')  # 적절한 태그와 클래스로 수정해야 함\n",
    "\n",
    "    for job in job_list:\n",
    "        # 공고 제목 추출\n",
    "        title_tag = job.find('h4', class_='sc-f0acadd4-7 hQClbs')\n",
    "        title = title_tag.text.strip() if title_tag else 'N/A'\n",
    "\n",
    "        # 회사 이름 추출\n",
    "        company_tag = job.find('p', class_='sc-f0acadd4-5 gCYsaW')\n",
    "        company_name = company_tag.text.strip() if company_tag else 'N/A'\n",
    "\n",
    "        #디테일 페이지로 가는 주소\n",
    "        detail_url = job.find('a', class_='sc-567718ed-0 jzTeOc')['href'] if job.find('a', class_='sc-567718ed-0 jzTeOc') else ''\n",
    "        detail_url = base_url + detail_url if detail_url else ''\n",
    "        \n",
    "        # 경력과 지역을 구분하여 가져오기\n",
    "        career_region_tags = job.find_all('p', class_='sc-f0acadd4-10')\n",
    "\n",
    "        career = 'N/A'\n",
    "        region = 'N/A'\n",
    "        for tag in career_region_tags:\n",
    "            text = tag.text.strip()\n",
    "            if '년' in text:  # \"년\"이 들어간 텍스트는 경력 정보\n",
    "                career = text\n",
    "            else:  # 나머지는 지역 정보로 간주\n",
    "                region = text\n",
    "\n",
    "        # 직무 카테고리 (사이트에 없다면 N/A로 처리)\n",
    "        category_name = 'N/A'\n",
    "        \n",
    "        # 기술 스택 (이 사이트에 해당 정보가 없으므로 'N/A'로 처리)\n",
    "        stack = 'N/A'\n",
    "        \n",
    "        # 출처 (플랫폼 이름)\n",
    "        platform_name = 'remember'\n",
    "\n",
    "        # 딕셔너리로 데이터 저장\n",
    "        jobs.append({\n",
    "            'title': title,\n",
    "            'company_name': company_name,\n",
    "            'detail_url': detail_url,\n",
    "            'end_date': 'N/A',  # end_date를 찾을 수 없는 경우 'N/A'\n",
    "            'platform_name': platform_name,\n",
    "            'category_name': category_name,\n",
    "            'stack': stack,\n",
    "            'region': region,\n",
    "            'career': career\n",
    "        })\n",
    "\n",
    "    # 스크롤을 아래로 내림\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    \n",
    "    # 새로운 데이터가 로드될 때까지 잠시 대기\n",
    "    time.sleep(2)\n",
    "\n",
    "    # 스크롤 후 새 높이를 측정\n",
    "    new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "    # 더 이상 로드되는 데이터가 없으면 종료\n",
    "    if new_height == last_height:\n",
    "        print(\"더 이상 스크롤 할 수 없습니다.\")\n",
    "        break\n",
    "\n",
    "    last_height = new_height\n",
    "    scroll_count += 1\n",
    "\n",
    "    # 원하는 횟수만큼 스크롤 후 멈추고 싶다면 조건 추가\n",
    "    #if scroll_count > 10:  # 예: 10번 스크롤 후 멈추기\n",
    "        #break\n",
    "print(scroll_count)\n",
    "# 브라우저 닫기\n",
    "driver.quit()\n",
    "\n",
    "# JSON 파일로 저장\n",
    "with open('remember_job_list.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(jobs, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"JSON 파일 저장 완료: 'remember_job_list.json'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crawling",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
